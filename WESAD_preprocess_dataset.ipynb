{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6173ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:146: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:149: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:146: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:149: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-1-e5cbf3886947>:146: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if norm_type is 'std':\n",
      "<ipython-input-1-e5cbf3886947>:149: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif norm_type is 'minmax':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing data for Subject S2 ...........\n",
      "Currently processing data for Subject S3 ...........\n",
      "Currently processing data for Subject S4 ...........\n",
      "Currently processing data for Subject S5 ...........\n",
      "Currently processing data for Subject S6 ...........\n",
      "Currently processing data for Subject S7 ...........\n",
      "Currently processing data for Subject S8 ...........\n",
      "Currently processing data for Subject S9 ...........\n",
      "Currently processing data for Subject S10 ...........\n",
      "Currently processing data for Subject S11 ...........\n",
      "Currently processing data for Subject S13 ...........\n",
      "Currently processing data for Subject S14 ...........\n",
      "Currently processing data for Subject S15 ...........\n",
      "Currently processing data for Subject S16 ...........\n",
      "Currently processing data for Subject S17 ...........\n",
      "Number of samples per class:\n",
      "baseline: 3819\n",
      "meditation: 2554\n",
      "stress: 2156\n",
      "amusement: 1200\n",
      "Initial Pre-Processing step complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy.signal as scisig\n",
    "import scipy.stats\n",
    "# import cvxEDA\n",
    "\n",
    "# E4 (wrist) Sampling Frequencies\n",
    "fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700}\n",
    "WINDOW_IN_SECONDS = 5\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0, 'meditation': 3}\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement', 3: 'meditation'}\n",
    "feat_names = None\n",
    "savePath = 'WESAD_pre_processed_data'\n",
    "subject_feature_path = '/subject_features'\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "if not os.path.exists(savePath + subject_feature_path):\n",
    "    os.makedirs(savePath + subject_feature_path)\n",
    "\n",
    "class SubjectData:\n",
    "\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'Dataset/WESAD/S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        data = self.data['signal']['wrist']\n",
    "        data.update({'Resp': self.data['signal']['chest']['Resp']})\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        return self.data['signal']['chest']\n",
    "\n",
    "    def extract_features(self):  # only wrist\n",
    "        results = \\\n",
    "            {\n",
    "                key: get_statistics(self.get_wrist_data()[key].flatten(), self.labels, key)\n",
    "                for key in self.wrist_keys\n",
    "            }\n",
    "        return results\n",
    "\n",
    "\n",
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/load_files.py\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = scisig.butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = scisig.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def get_slope(series):\n",
    "    linreg = scipy.stats.linregress(np.arange(len(series)), series )\n",
    "    slope = linreg[0]\n",
    "    return slope\n",
    "\n",
    "def get_window_stats(data, label=-1):\n",
    "    mean_features = np.mean(data)\n",
    "    std_features = np.std(data)\n",
    "    min_features = np.amin(data)\n",
    "    max_features = np.amax(data)\n",
    "\n",
    "    features = {'mean': mean_features, 'std': std_features, 'min': min_features, 'max': max_features,\n",
    "                'label': label}\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_net_accel(data):\n",
    "    return (data['ACC_x'] ** 2 + data['ACC_y'] ** 2 + data['ACC_z'] ** 2).apply(lambda x: np.sqrt(x))\n",
    "\n",
    "\n",
    "def get_peak_freq(x):\n",
    "    f, Pxx = scisig.periodogram(x, fs=8)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    peak_freq = psd_dict[max(psd_dict.keys())]\n",
    "    return peak_freq\n",
    "\n",
    "\n",
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/AccelerometerFeatureExtractionScript.py\n",
    "def filterSignalFIR(eda, cutoff=0.4, numtaps=64):\n",
    "    f = cutoff / (fs_dict['ACC'] / 2.0)\n",
    "    FIR_coeff = scisig.firwin(numtaps, f)\n",
    "\n",
    "    return scisig.lfilter(FIR_coeff, 1, eda)\n",
    "\n",
    "\n",
    "def compute_features(e4_data_dict, labels, norm_type=None):\n",
    "    # Dataframes for each sensor type\n",
    "    eda_df = pd.DataFrame(e4_data_dict['EDA'], columns=['EDA'])\n",
    "    bvp_df = pd.DataFrame(e4_data_dict['BVP'], columns=['BVP'])\n",
    "    acc_df = pd.DataFrame(e4_data_dict['ACC'], columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "    temp_df = pd.DataFrame(e4_data_dict['TEMP'], columns=['TEMP'])\n",
    "    label_df = pd.DataFrame(labels, columns=['label'])\n",
    "    resp_df = pd.DataFrame(e4_data_dict['Resp'], columns=['Resp'])\n",
    "\n",
    "    # Filter EDA\n",
    "    eda_df['EDA'] = butter_lowpass_filter(eda_df['EDA'], 1.0, fs_dict['EDA'], 6)\n",
    "\n",
    "    # Filter ACM\n",
    "    for _ in acc_df.columns:\n",
    "        acc_df[_] = filterSignalFIR(acc_df.values)\n",
    "\n",
    "    # Adding indices for combination due to differing sampling frequencies\n",
    "    eda_df.index = [(1 / fs_dict['EDA']) * i for i in range(len(eda_df))]\n",
    "    bvp_df.index = [(1 / fs_dict['BVP']) * i for i in range(len(bvp_df))]\n",
    "    acc_df.index = [(1 / fs_dict['ACC']) * i for i in range(len(acc_df))]\n",
    "    temp_df.index = [(1 / fs_dict['TEMP']) * i for i in range(len(temp_df))]\n",
    "    label_df.index = [(1 / fs_dict['label']) * i for i in range(len(label_df))]\n",
    "    resp_df.index = [(1 / fs_dict['Resp']) * i for i in range(len(resp_df))]\n",
    "\n",
    "    # Change indices to datetime\n",
    "    eda_df.index = pd.to_datetime(eda_df.index, unit='s')\n",
    "    bvp_df.index = pd.to_datetime(bvp_df.index, unit='s')\n",
    "    temp_df.index = pd.to_datetime(temp_df.index, unit='s')\n",
    "    acc_df.index = pd.to_datetime(acc_df.index, unit='s')\n",
    "    label_df.index = pd.to_datetime(label_df.index, unit='s')\n",
    "    resp_df.index = pd.to_datetime(resp_df.index, unit='s')\n",
    "\n",
    "        \n",
    "    # Combined dataframe - not used yet\n",
    "    df = eda_df.join(bvp_df, how='outer')\n",
    "    df = df.join(temp_df, how='outer')\n",
    "    df = df.join(acc_df, how='outer')\n",
    "    df = df.join(resp_df, how='outer')\n",
    "    df = df.join(label_df, how='outer')\n",
    "    df['label'] = df['label'].fillna(method='bfill')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if norm_type is 'std':\n",
    "        # std norm\n",
    "        df = (df - df.mean()) / df.std()\n",
    "    elif norm_type is 'minmax':\n",
    "        # minmax norm\n",
    "        df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    # Groupby\n",
    "    grouped = df.groupby('label')\n",
    "    baseline = grouped.get_group(1)\n",
    "    stress = grouped.get_group(2)\n",
    "    amusement = grouped.get_group(3)\n",
    "    meditation = grouped.get_group(4)\n",
    "    return grouped, baseline, stress, amusement, meditation\n",
    "\n",
    "\n",
    "def get_samples(data, n_windows, label):\n",
    "    global feat_names\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    samples = []\n",
    "    # Using label freq (700 Hz) as our reference frequency due to it being the largest\n",
    "    # and thus encompassing the lesser ones in its resolution.\n",
    "    window_len = fs_dict['label'] * WINDOW_IN_SECONDS\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        # Get window of data\n",
    "        w = data[window_len * i: window_len * (i + 1)]\n",
    "\n",
    "        # Add/Calc rms acc\n",
    "        # w['net_acc'] = get_net_accel(w)\n",
    "        w = pd.concat([w, get_net_accel(w)])\n",
    "        #w.columns = ['net_acc', 'ACC_x', 'ACC_y', 'ACC_z', 'BVP',\n",
    "          #           'EDA', 'EDA_phasic', 'EDA_smna', 'EDA_tonic', 'TEMP',\n",
    "            #         'label']\n",
    "        cols = list(w.columns)\n",
    "        cols[0] = 'net_acc'\n",
    "        w.columns = cols\n",
    "        \n",
    "        # Calculate stats for window\n",
    "        wstats = get_window_stats(data=w, label=label)\n",
    "\n",
    "        # Seperating sample and label\n",
    "        x = pd.DataFrame(wstats).drop('label', axis=0)\n",
    "        y = x['label'][0]\n",
    "        x.drop('label', axis=1, inplace=True)\n",
    "\n",
    "        if feat_names is None:\n",
    "            feat_names = []\n",
    "            for row in x.index:\n",
    "                for col in x.columns:\n",
    "                    feat_names.append('_'.join([row, col]))\n",
    "\n",
    "        # sample df\n",
    "        wdf = pd.DataFrame(x.values.flatten()).T\n",
    "        wdf.columns = feat_names\n",
    "        wdf = pd.concat([wdf, pd.DataFrame({'label': y}, index=[0])], axis=1)\n",
    "        \n",
    "        # More feats\n",
    "        wdf['BVP_peak_freq'] = get_peak_freq(w['BVP'].dropna())\n",
    "        wdf['TEMP_slope'] = get_slope(w['TEMP'].dropna())\n",
    "        samples.append(wdf)\n",
    "\n",
    "    return pd.concat(samples)\n",
    "\n",
    "\n",
    "def make_patient_data(subject_id):\n",
    "    global savePath\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    # Make subject data object for Sx\n",
    "    subject = SubjectData(main_path='', subject_number=subject_id)\n",
    "\n",
    "    # Empatica E4 data - now with resp\n",
    "    e4_data_dict = subject.get_wrist_data()\n",
    "\n",
    "    # norm type\n",
    "    norm_type = None\n",
    "\n",
    "    # The 3 classes we are classifying\n",
    "    grouped, baseline, stress, amusement, meditation = compute_features(e4_data_dict, subject.labels, norm_type)\n",
    "\n",
    "    # print(f'Available windows for {subject.name}:')\n",
    "    n_baseline_wdws = int(len(baseline) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    n_stress_wdws = int(len(stress) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    n_amusement_wdws = int(len(amusement) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    n_meditation_wdws = int(len(meditation) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    # print(f'Baseline: {n_baseline_wdws}\\nStress: {n_stress_wdws}\\nAmusement: {n_amusement_wdws}\\n')\n",
    "\n",
    "    #\n",
    "    baseline_samples = get_samples(baseline, n_baseline_wdws, 1)\n",
    "    # Downsampling\n",
    "    # baseline_samples = baseline_samples[::2]\n",
    "    stress_samples = get_samples(stress, n_stress_wdws, 2)\n",
    "    amusement_samples = get_samples(amusement, n_amusement_wdws, 0)\n",
    "    meditation_samples = get_samples(meditation, n_meditation_wdws, 3)\n",
    "\n",
    "    all_samples = pd.concat([baseline_samples, stress_samples, amusement_samples, meditation_samples])\n",
    "    all_samples = pd.concat([all_samples.drop('label', axis=1), pd.get_dummies(all_samples['label'])], axis=1)\n",
    "    # Selected Features\n",
    "    # all_samples = all_samples[['EDA_mean', 'EDA_std', 'EDA_min', 'EDA_max',\n",
    "    #                          'BVP_mean', 'BVP_std', 'BVP_min', 'BVP_max',\n",
    "    #                        'TEMP_mean', 'TEMP_std', 'TEMP_min', 'TEMP_max',\n",
    "    #                        'net_acc_mean', 'net_acc_std', 'net_acc_min', 'net_acc_max',\n",
    "    #                        0, 1, 2]]\n",
    "    # Save file as csv (for now)\n",
    "    all_samples.to_csv(f'{savePath}{subject_feature_path}/S{subject_id}_features.csv')\n",
    "\n",
    "    # Does this save any space?\n",
    "    subject = None\n",
    "\n",
    "\n",
    "def combine_files(subjects):\n",
    "    df_list = []\n",
    "    for s in subjects:\n",
    "        df = pd.read_csv(f'{savePath}{subject_feature_path}/S{s}_features.csv', index_col=0)\n",
    "        df['subject'] = s\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str) + df['3'].astype(str)).apply(lambda x: x.index('1'))\n",
    "    df.drop(['0', '1', '2', '3'], axis=1, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.to_csv(f'{savePath}/combined_features.csv')\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "    print('Number of samples per class:')\n",
    "    for label, number in zip(counts.index, counts.values):\n",
    "        print(f'{int_to_label[label]}: {number}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "\n",
    "    for patient in subject_ids:\n",
    "        print(f'Currently processing data for Subject S{patient} ...........')\n",
    "        make_patient_data(patient)\n",
    "\n",
    "    combine_files(subject_ids)\n",
    "    print('Initial Pre-Processing step complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c168b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
